# Data Quality and Governance for Customer Transactions

## Project Goal

This project demonstrates a complete data quality and governance workflow. It takes a raw transactional dataset, profiles it, cleans it according to a defined set of rules, and generates a comprehensive data quality report in HTML.

## Project Description

The process is orchestrated by a main script and broken down into logical components:

1.  **Data Acquisition & Extraction**: This project utilizes the "Online Retail II" dataset, a real-world transactional dataset sourced from the UCI Machine Learning Repository. You can find more details about it [here](https://archive.ics.uci.edu/dataset/502/online+retail+ii). The initial script is responsible for extracting this data from its compressed format.
2.  **Data Cleaning & Profiling**: Using the `pandas` library, the script performs a series of data quality checks and cleaning operations, including:
    *   Removing rows with missing essential data (CustomerID, Description).
    *   Identifying and removing canceled transactions.
    *   Dropping duplicate records.
    *   Standardizing data types (e.g., converting dates).
3.  **Reporting**: After cleaning, the project uses the `ydata-profiling` library to automatically generate a detailed, interactive HTML report. This report provides a deep dive into the dataset's variables, correlations, missing values, and more, allowing for a thorough analysis of the data's quality.

## How to Run the Project

### Prerequisites

*   Python 3.x
*   pip (Python package installer)

### 1. Installation

First, install the necessary Python libraries by running the following command in your terminal from the project's root directory:

```bash
pip install -r requirements.txt
```

### 2. Execution

To run the entire workflow from start to finish, execute the main script from the project's root directory:

```bash
python src/main.py
```

The script will provide status updates in the console as it moves through the extraction, cleaning, and reporting stages.

## Project Structure

```
.
├── data/                      # Directory for all data files.
│   ├── online_retail_II.xlsx      # The raw, original dataset.
│   ├── online_retail_II_cleaned.csv # The cleaned dataset generated by the script.
│   └── online+retail+ii.zip       # The original downloaded zip archive.
├── src/                       # Source code directory.
│   ├── clean_and_profile.py     # Script for cleaning and profiling the data.
│   ├── download_data.py         # Script to handle data extraction.
│   ├── generate_report.py       # Script to generate the final HTML report.
│   └── main.py                  # The main script to orchestrate the project.
├── data_quality_report.html   # The final, interactive data quality report.
└── requirements.txt           # A list of the Python libraries required.
```

## Output

After running the project, you will find two primary outputs:

1.  **Cleaned Data**: A file named `online_retail_II_cleaned.csv` located in the `data/` directory. This file contains the dataset after all the quality rules have been applied.
2.  **Data Quality Report**: A file named `data_quality_report.html` in the root directory. You can open this file in any web browser to view the interactive and detailed data profile.
